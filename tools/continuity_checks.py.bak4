from __future__ import annotations

import argparse
import hashlib
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Any

from _common import (
    chapter_id,
    get_repo_root,
    iso_today,
    load_json,
    normalize_chapter_id,
    parse_chapter_num_from_id,
    read_text,
    safe_write_text,
    setup_logger,
)

_TITLE_LINE_RE = re.compile(r"^##\s+《.+》\s*$")

# 叙事质量检查：默认不把“新结构字段缺失”直接判 FAIL，以兼容旧章纲。
# 等 ChapterPlanner/Drafter 升级后，你可以把它改为 True，直接硬卡口。
STRICT_NARRATIVE_QA = False


def _extract_plan_chapter_title(plan_text: str) -> str | None:
    m = re.search(r"^chapter_title:\s*《(.+?)》\s*$", plan_text, re.MULTILINE)
    return m.group(1).strip() if m else None


def _extract_manuscript_title(chapter_text: str) -> str | None:
    lines = chapter_text.splitlines()
    if len(lines) < 2:
        return None
    m = re.match(r"^##\s+《(.+?)》\s*$", lines[1].strip())
    return m.group(1).strip() if m else None


def _extract_section(plan_text: str, start_header: str) -> str:
    """
    从 Markdown 中抽取某个 header 到下一个 header 之间的内容。
    start_header 需与行内容完全匹配，例如 '### Open Loops'。
    规则：从 start_header 下一行开始，直到遇到下一行以 '#' 开头的 header 为止。
    """
    lines = plan_text.splitlines()
    start_idx = -1
    for i, line in enumerate(lines):
        if line.strip() == start_header:
            start_idx = i
            break
    if start_idx < 0:
        return ""

    buf: list[str] = []
    for line in lines[start_idx + 1 :]:
        if line.lstrip().startswith("#"):
            break
        buf.append(line)
    return "\n".join(buf).strip()


def _extract_anchor_phrases(plan_text: str) -> list[str]:
    # 兼容两种写法：
    # 1) 单独字段行：anchor_phrase: xxxx
    # 2) 行内字典：..., anchor_phrase: xxxx, ...
    phrases: list[str] = []
    for m in re.finditer(r"anchor_phrase:\s*([^\n,}]+)", plan_text):
        v = m.group(1).strip().strip('"').strip("'")
        if v:
            phrases.append(v)

    # 去重保持顺序
    seen: set[str] = set()
    out: list[str] = []
    for p in phrases:
        if p not in seen:
            seen.add(p)
            out.append(p)
    return out


def _count_scenes(plan_text: str) -> int:
    # 优先统计 no: N
    nos = re.findall(r"\bno:\s*(\d+)\b", plan_text)
    if nos:
        return len(nos)
    # 退化：按 setting: 行估计
    return len(re.findall(r"\bsetting:\s*\S", plan_text))


def _has_all_skeleton_fields(plan_text: str) -> tuple[bool, list[str]]:
    required = ["hook_scene_no:", "push_scene_nos:", "escalation_scene_no:", "ending_hook_scene_no:"]
    missing = [k for k in required if k not in plan_text]
    return (len(missing) == 0, missing)


def _extract_style_locks(style_text: str) -> dict[str, str]:
    locks: dict[str, str] = {}
    for line in style_text.splitlines():
        line = line.strip()
        if line.startswith("- 人称："):
            locks["person"] = line.split("：", 1)[1].strip()
        if line.startswith("- 视角："):
            locks["pov"] = line.split("：", 1)[1].strip()
        if line.startswith("- 时态："):
            locks["tense"] = line.split("：", 1)[1].strip()
    return locks


def _extract_character_names(characters_yaml_text: str) -> set[str]:
    names: set[str] = set()
    for line in characters_yaml_text.splitlines():
        line = line.strip()
        if line.startswith("name:"):
            v = line.split(":", 1)[1].strip()
            if v and v.upper() != "TBD":
                names.add(v)
    return names


def _detect_capitalized_names(chapter_text: str) -> set[str]:
    return set(re.findall(r"\b[A-Z][a-z]{2,}\b", chapter_text))


def _planned_open_loop_ids(chapter_plan_text: str, known_loop_ids: set[str]) -> list[str]:
    """
    只在 '### Open Loops' 段落内匹配，避免 loop id 在说明文字/示例里出现导致误判。
    若未找到该段落，则回退到全文匹配（兼容旧格式）。
    """
    sec = _extract_section(chapter_plan_text, start_header="### Open Loops")
    target_text = sec if sec else chapter_plan_text
    return [loop_id for loop_id in sorted(known_loop_ids) if loop_id and (loop_id in target_text)]


def _load_optional_json(path: Path) -> dict[str, Any] | None:
    if not path.exists():
        return None
    try:
        obj = load_json(path)
    except Exception:  # noqa: BLE001
        return None
    return obj if isinstance(obj, dict) else None


def _canon_change_watch(root: Path, run_dir: Path) -> tuple[list[str], list[str]]:
    manifest_path = run_dir / "canon_manifest.json"
    if not manifest_path.exists():
        return [], []

    try:
        manifest = load_json(manifest_path)
    except Exception:  # noqa: BLE001
        return [], []
    files = manifest.get("files", [])
    if not isinstance(files, list):
        return [], []

    append_like: list[str] = []
    rewrite_like: list[str] = []

    for entry in files:
        if not isinstance(entry, dict):
            continue
        rel = entry.get("path")
        old_sha = entry.get("sha256")
        old_head = entry.get("head_sha256")
        head_bytes = entry.get("head_bytes")
        if not isinstance(rel, str) or not isinstance(old_sha, str) or not isinstance(old_head, str):
            continue
        if not isinstance(head_bytes, int) or head_bytes <= 0:
            head_bytes = 32768

        path = root / rel
        if not path.exists():
            continue

        data = path.read_bytes()
        new_sha = hashlib.sha256(data).hexdigest()
        if new_sha == old_sha:
            continue

        new_head = hashlib.sha256(data[:head_bytes]).hexdigest()
        if new_head == old_head:
            append_like.append(rel)
        else:
            rewrite_like.append(rel)

    return append_like, rewrite_like


def _calc_body_chars(chapter_text: str, chap: str) -> int:
    lines = chapter_text.splitlines()
    start = 0
    if lines and lines[0].strip() == f"# {chap}":
        start = 1
        if start < len(lines):
            next_line = lines[start].strip()
            if next_line.startswith("## 第") or _TITLE_LINE_RE.match(next_line):
                start += 1
    body_lines = []
    for line in lines[start:]:
        if line.lstrip().startswith("#"):
            continue
        body_lines.append(line)
    body_text = "".join(body_lines)
    body_text = "".join(ch for ch in body_text if not ch.isspace())
    return len(body_text)


def _check_title_lines(chapter_text: str, chap: str) -> tuple[bool, str]:
    lines = chapter_text.splitlines()
    if not lines:
        return False, "正文为空，无法校验标题行。"
    line1 = lines[0].strip() if lines else ""
    if line1 != f"# {chap}":
        return False, f"首行应为 '# {chap}'，实际为：{line1 or '（空）'}"
    line2 = lines[1].strip() if len(lines) > 1 else ""
    if not _TITLE_LINE_RE.match(line2):
        return False, f"第二行应为 '## 《标题》'，实际为：{line2 or '（空）'}"
    return True, ""


def _scan_placeholders(text: str) -> list[str]:
    issues: list[str] = []
    if re.search(r"\?{3,}", text):
        issues.append("连续问号(???)")
    if re.search(r"\bTBD\b", text, re.IGNORECASE):
        issues.append("TBD")
    if re.search(r"\bTODO\b", text, re.IGNORECASE):
        issues.append("TODO")
    if "待补全" in text:
        issues.append("待补全")
    if "\ufffd" in text:
        issues.append("替换字符(�)")
    return issues


def _scan_empty_plan_fields(plan_text: str) -> list[str]:
    issues: list[str] = []
    for line in plan_text.splitlines():
        stripped = line.strip()
        if stripped.startswith("ending_hook_payload:") or stripped.startswith("- ending_hook_payload:"):
            value = stripped.split(":", 1)[1].strip()
            if not value:
                issues.append("ending_hook_payload 为空")
        if stripped.startswith("payload:") or stripped.startswith("- payload:"):
            value = stripped.split(":", 1)[1].strip()
            if not value:
                issues.append("ending_hook payload 为空")
    for field in ("objective", "conflict", "reveal", "exit_hook"):
        if re.search(rf"{field}:\s*(?=,|}})", plan_text):
            issues.append(f"scenes.{field} 为空")
    return issues


def _format_mtime(ts: float | None) -> str:
    if ts is None:
        return "无"
    return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")


def _has_failure(failures: list[dict[str, str]], rule: str) -> bool:
    return any(item.get("rule") == rule for item in failures)


def check_artifacts_in_sync(
    root: Path,
    run_dir: Path,
    chapter_id: str,
    failures: list[dict[str, str]],
) -> dict[str, Any]:
    artifacts = {
        "manuscript": root / "manuscript" / f"{chapter_id}.md",
        "changelog": run_dir / "changelog.md",
        "chapter_summary": root / "recap" / "chapter_summaries" / f"{chapter_id}.md",
        "state_patch": root / "state" / "state_patch.json",
    }
    labels = {
        "manuscript": "正文",
        "changelog": "变更日志",
        "chapter_summary": "章节摘要",
        "state_patch": "状态补丁",
    }

    status: dict[str, dict[str, Any]] = {}
    missing: list[str] = []
    for key, path in artifacts.items():
        if path.exists():
            mtime = path.stat().st_mtime
        else:
            mtime = None
            missing.append(key)
        status[key] = {"path": path, "mtime": mtime}

    for key in missing:
        rule = f"artifact_missing: {key}"
        if _has_failure(failures, rule):
            continue
        failures.append(
            {
                "rule": rule,
                "evidence": f"缺失文件：{status[key]['path'].as_posix()}",
                "fix": "重跑 Archivist（Step 3）生成 recap/state/changelog 后再 QA。",
            }
        )

    out_of_sync: list[str] = []
    manuscript_mtime = status["manuscript"]["mtime"]
    if manuscript_mtime is not None:
        for key in ("changelog", "chapter_summary", "state_patch"):
            other_mtime = status[key]["mtime"]
            if other_mtime is not None and manuscript_mtime > other_mtime:
                out_of_sync.append(key)

    if out_of_sync and not _has_failure(failures, "artifacts_out_of_sync"):
        details = []
        for key in out_of_sync:
            details.append(f"{labels.get(key, key)}={_format_mtime(status[key]['mtime'])}")
        failures.append(
            {
                "rule": "artifacts_out_of_sync",
                "evidence": ("正文更新时间 " f"{_format_mtime(manuscript_mtime)} 晚于：{', '.join(details)}"),
                "fix": "重跑 Archivist（Step 3）同步 recap/state/changelog。",
            }
        )

    report_lines = []
    for key in ("manuscript", "chapter_summary", "state_patch", "changelog"):
        path = status[key]["path"]
        mtime = status[key]["mtime"]
        label = labels.get(key, key)
        if mtime is None:
            report_lines.append(f"- {label}: {path.as_posix()} | 缺失")
        else:
            report_lines.append(f"- {label}: {path.as_posix()} | 时间：{_format_mtime(mtime)}")

    return {
        "missing": missing,
        "out_of_sync": out_of_sync,
        "status": status,
        "report_lines": report_lines,
    }


def _build_resync_instructions(
    run_date: str,
    chapter_id: str,
    sync_info: dict[str, Any],
) -> str:
    missing = sync_info.get("missing", [])
    out_of_sync = sync_info.get("out_of_sync", [])
    status = sync_info.get("status", {})

    lines: list[str] = []
    lines.append(f"# 归档同步指引 — {run_date} — {chapter_id}")
    lines.append("")
    lines.append("## 失败原因")
    if missing:
        missing_paths = [status[key]["path"].as_posix() for key in missing if key in status]
        lines.append(f"- 缺失文件：{', '.join(missing_paths)}")
    if out_of_sync:
        manuscript_mtime = status["manuscript"]["mtime"]
        newer_than = []
        for key in out_of_sync:
            newer_than.append(
                f"{status[key]['path'].as_posix()} ({_format_mtime(status[key]['mtime'])})"
            )
        lines.append(
            "- 不同步：正文版本更新晚于 "
            + ", ".join(newer_than)
            + f"（正文更新时间：{_format_mtime(manuscript_mtime)}）"
        )
    if not missing and not out_of_sync:
        lines.append("- 未检测到缺失/不同步（此文件仅供参考）。")

    lines.append("")
    lines.append("## VSCode Codex 指令（可复制粘贴）")
    lines.append("```")
    lines.append("你是 Archivist。请严格按仓库规范工作，只写指定文件，路径必须完全一致，不要改名，不要输出到别处。")
    lines.append("")
    lines.append("必须先阅读这些文件作为上下文：")
    lines.append(f"- manuscript/{chapter_id}.md（以此为最新事实来源）")
    lines.append(f"- runs/{run_date}/chapter_plan.md")
    lines.append("- state/current_state.json")
    lines.append("- recap/rolling_recap.md")
    lines.append("- prompts/roles/archivist.md")
    lines.append("- canon/style/style_guide.md")
    lines.append("")
    lines.append("任务：重新生成并写入以下文件（全部必须写入/覆盖旧内容）：")
    lines.append(f"1) recap/chapter_summaries/{chapter_id}.md")
    lines.append("2) recap/rolling_recap.md")
    lines.append("3) state/state_patch.json（JSON 对象，仅 delta；必须包含 meta.current_chapter）")
    lines.append(f"4) runs/{run_date}/changelog.md（追加 synced_at 时间戳）")
    lines.append("")
    lines.append("硬性要求：")
    lines.append("- 只允许写入上述四个路径，不得修改其他文件")
    lines.append("- state/state_patch.json 必须是严格 JSON 对象，仅 delta")
    lines.append(f"- meta.current_chapter 必须为 \"{chapter_id}\"（字符串）")
    lines.append("- 不生成任何小说正文")
    lines.append("")
    lines.append("同步完成后请运行：")
    lines.append(f"python tools/continuity_checks.py --date {run_date} --chapter {chapter_id}")
    lines.append("```")
    lines.append("")
    return "\n".join(lines)


def main() -> int:
    parser = argparse.ArgumentParser(
        description="Run continuity and hard-rule checks; write runs/YYYY-MM-DD/qa_report.md."
    )
    parser.add_argument("--date", default=iso_today(), help="Run date (YYYY-MM-DD). Default: today.")
    parser.add_argument("--chapter", type=str, required=True, help="Chapter id (chNNN) or number (NNN).")
    parser.add_argument("--verbose", action="store_true", help="Verbose logging.")
    args = parser.parse_args()

    root = get_repo_root()
    run_dir = root / "runs" / args.date
    run_dir.mkdir(parents=True, exist_ok=True)
    log = setup_logger(
        "continuity_checks", log_file=run_dir / "continuity_checks.log", verbose=args.verbose
    )

    try:
        chap = normalize_chapter_id(args.chapter)
    except ValueError as e:
        log.error("Invalid --chapter: %s (%s)", args.chapter, e)
        return 2

    if parse_chapter_num_from_id(chap) is None:
        log.error("Invalid chapter id after normalization: %s", chap)
        return 2
    chapter_path = root / "manuscript" / f"{chap}.md"
    summary_path = root / "recap" / "chapter_summaries" / f"{chap}.md"
    plan_path = run_dir / "chapter_plan.md"
    patch_path = root / "state" / "state_patch.json"
    qa_path = run_dir / "qa_report.md"

    failures: list[dict[str, str]] = []
    warnings: list[dict[str, str]] = []
    infos: list[str] = []
    placeholder_report_lines: list[str] = []

    body_chars: int | None = None
    if not chapter_path.exists():
        failures.append(
            {
                "rule": "artifact_missing: manuscript",
                "evidence": f"缺失文件：{chapter_path.as_posix()}",
                "fix": "先生成 `manuscript/chNNN.md`。",
            }
        )
        chapter_text = ""
    else:
        chapter_text = read_text(chapter_path)
        title_ok, title_issue = _check_title_lines(chapter_text, chap)
        if not title_ok:
            failures.append(
                {
                    "rule": "title_missing_or_invalid",
                    "evidence": title_issue,
                    "fix": "确保正文首行 '# chNNN'，第二行 '## 《标题》'，标题来自 chapter_plan.md 的 chapter_title。",
                }
            )
        body_chars = _calc_body_chars(chapter_text, chap)
        if body_chars < 3000:
            failures.append(
                {
                    "rule": "BodyTooShort",
                    "evidence": f"正文字符不足：{body_chars}/3000",
                    "fix": "扩写正文至 >=3000 字符（不含标题与空白）。",
                }
            )
        if "TBD" in chapter_text:
            failures.append(
                {
                    "rule": "no_TBD_in_manuscript",
                    "evidence": "正文中出现 'TBD' 占位符。",
                    "fix": "将所有占位符替换为实际正文内容。",
                }
            )
        manuscript_placeholder_issues = [issue for issue in _scan_placeholders(chapter_text) if issue != "TBD"]
        if manuscript_placeholder_issues:
            failures.append(
                {
                    "rule": "manuscript_placeholders_or_garbled",
                    "evidence": f"正文中出现占位/乱码：{', '.join(manuscript_placeholder_issues)}",
                    "fix": "重跑 Step2（正文+润色），确保 UTF-8 保存且不得出现 ???/TODO/乱码。",
                }
            )

    style_path = root / "canon" / "style" / "style_guide.md"
    style_text = read_text(style_path) if style_path.exists() else ""
    locks = _extract_style_locks(style_text)

    state_path = root / "state" / "current_state.json"
    state_obj = _load_optional_json(state_path) or {}
    meta = state_obj.get("meta", {}) if isinstance(state_obj.get("meta", {}), dict) else {}
    meta_pov = str(meta.get("pov", "")).strip()
    pov_lock = meta_pov if meta_pov and meta_pov.upper() != "TBD" else locks.get("pov", "")

    if not pov_lock or pov_lock.upper() == "TBD":
        warnings.append(
            {
                "topic": "pov_lock_missing",
                "evidence": "state.meta.pov / style_guide 的 POV 为 TBD 或缺失。",
                "suggestion": "请在 `state/current_state.json.meta.pov` 与 `canon/style/style_guide.md` 中设置具体 POV 锁。",
            }
        )
    else:
        first_person_markers = chapter_text.count("我") + chapter_text.count("我们")
        if ("第三" in pov_lock) or ("third" in pov_lock.lower()):
            if first_person_markers >= 20:
                failures.append(
                    {
                        "rule": "pov_consistency_third_person",
                        "evidence": f"视角锁为第三人称但第一人称标记过多（count={first_person_markers}）。",
                        "fix": "修订叙述，移除第一人称视角漂移。",
                    }
                )

    state_current_raw = meta.get("current_chapter")
    state_current = None
    state_current_note = "无"
    if state_current_raw is not None:
        try:
            state_current = normalize_chapter_id(state_current_raw)
            state_current_note = state_current
        except ValueError:
            state_current_note = f"无效：{state_current_raw!r}"
            warnings.append(
                {
                    "topic": "state_current_chapter_invalid",
                    "evidence": f"state.meta.current_chapter 无效：{state_current_raw!r}",
                    "suggestion": "确保 `state/current_state.json` 使用如 ch001 的章节编号格式。",
                }
            )

    patch_obj = _load_optional_json(patch_path)
    patch_current = None
    patch_current_note = "缺失/无效"
    if patch_obj is None:
        if not _has_failure(failures, "state_patch_missing_for_chapter_check"):
            failures.append(
                {
                    "rule": "state_patch_missing_for_chapter_check",
                    "evidence": f"缺失或无效：{patch_path.as_posix()}",
                    "fix": "先执行 Step3（Archivist）生成正确的 state/state_patch.json。",
                }
            )
    else:
        patch_meta = patch_obj.get("meta", {}) if isinstance(patch_obj.get("meta", {}), dict) else {}
        patch_current_raw = patch_meta.get("current_chapter")
        if patch_current_raw is None:
            failures.append(
                {
                    "rule": "chapter_id_mismatch",
                    "evidence": "state_patch.meta.current_chapter 缺失。",
                    "fix": "重跑 Step3（Archivist）生成包含 meta.current_chapter 的 state_patch。",
                }
            )
            patch_current_note = "缺失"
        else:
            try:
                patch_current = normalize_chapter_id(patch_current_raw)
                patch_current_note = patch_current
            except ValueError:
                failures.append(
                    {
                        "rule": "chapter_id_mismatch",
                        "evidence": f"state_patch.meta.current_chapter 无效：{patch_current_raw!r}",
                        "fix": "重跑 Step3（Archivist）生成正确的 state_patch。",
                    }
                )
                patch_current_note = f"无效：{patch_current_raw!r}"
            else:
                if patch_current != chap:
                    failures.append(
                        {
                            "rule": "chapter_id_mismatch",
                            "evidence": f"state_patch.meta.current_chapter={patch_current} 与目标章节 {chap} 不一致。",
                            "fix": "重跑 Step3（Archivist）生成与目标章节一致的 state_patch。",
                        }
                    )

    infos.append(f"目标章节：{chap}")
    infos.append(f"state_patch.meta.current_chapter：{patch_current_note}")
    if state_current_note == "无":
        infos.append("state.current_state.meta.current_chapter：无")
    elif state_current and state_current != chap:
        infos.append(f"state.current_state.meta.current_chapter：{state_current}（允许落后，属正常）")
    else:
        infos.append(f"state.current_state.meta.current_chapter：{state_current_note}")

    characters_path = root / "canon" / "characters" / "characters.yaml"
    known_names = _extract_character_names(read_text(characters_path) if characters_path.exists() else "")
    if known_names:
        unknown_caps = _detect_capitalized_names(chapter_text) - known_names
        if unknown_caps:
            warnings.append(
                {
                    "topic": "unknown_capitalized_names",
                    "evidence": f"发现可能未知的人名：{', '.join(sorted(list(unknown_caps))[:20])}",
                    "suggestion": "如为主要角色，请追加到 `canon/characters/characters.yaml`（只追加）。",
                }
            )
    else:
        warnings.append(
            {
                "topic": "character_catalog_TBD",
                "evidence": "canon/characters/characters.yaml 中未发现具体角色名（均为 TBD）。",
                "suggestion": "补全 characters.yaml 的角色名以增强 QA 校验。",
            }
        )

    in_story_date = str(meta.get("in_story_date", "")).strip()
    if not in_story_date or in_story_date.upper() == "TBD":
        warnings.append(
            {
                "topic": "timeline_anchor_missing",
                "evidence": "state.meta.in_story_date 为 TBD 或缺失。",
                "suggestion": "设置 ISO 风格的 in_story_date 并保持单调递增；通过 state_patch 更新。",
            }
        )

    open_loops = state_obj.get("open_loops", []) if isinstance(state_obj.get("open_loops", []), list) else []
    known_loop_ids = {str(x.get("id")) for x in open_loops if isinstance(x, dict) and x.get("id")}

    if not plan_path.exists():
        failures.append(
            {
                "rule": "artifact_missing: chapter_plan",
                "evidence": f"缺失文件：{plan_path.as_posix()}",
                "fix": "先生成 `runs/YYYY-MM-DD/chapter_plan.md`。",
            }
        )
        planned: list[str] = []
        placeholder_report_lines.append("- chapter_plan: 缺失")
    else:
        plan_text = read_text(plan_path)
        planned = _planned_open_loop_ids(plan_text, known_loop_ids)

        # --- 叙事质量增强检查（与 ChapterPlanner/Drafter 的新字段闭环） ---
        plan_title = _extract_plan_chapter_title(plan_text)
        if not plan_title:
            warnings.append(
                {
                    "topic": "plan_chapter_title_missing",
                    "evidence": "chapter_plan.md 未找到 chapter_title: 《…》 行。",
                    "suggestion": "更新 ChapterPlanner，确保输出 chapter_title 字段（并用《》包住）。",
                }
            )
        else:
            ms_title = _extract_manuscript_title(chapter_text)
            if ms_title and (ms_title != plan_title):
                failures.append(
                    {
                        "rule": "title_mismatch_with_plan",
                        "evidence": f"章纲标题=《{plan_title}》，但正文标题=《{ms_title}》。",
                        "fix": "以 chapter_plan.md 的 chapter_title 为准，修正 manuscript 第二行标题，或重跑 Step2。",
                    }
                )

        has_any_skeleton = any(
            k in plan_text
            for k in ["hook_scene_no:", "push_scene_nos:", "escalation_scene_no:", "ending_hook_scene_no:"]
        )
        if has_any_skeleton:
            ok, missing = _has_all_skeleton_fields(plan_text)
            if not ok:
                msg = f"章纲出现部分 Chapter Skeleton 字段，但缺失：{', '.join(missing)}"
                if STRICT_NARRATIVE_QA:
                    failures.append(
                        {
                            "rule": "chapter_skeleton_incomplete",
                            "evidence": msg,
                            "fix": "在 chapter_plan.md 补齐 hook_scene_no/push_scene_nos/escalation_scene_no/ending_hook_scene_no。",
                        }
                    )
                else:
                    warnings.append(
                        {
                            "topic": "chapter_skeleton_incomplete",
                            "evidence": msg,
                            "suggestion": "建议升级 ChapterPlanner 输出完整四段骨架映射，后续可将其设为硬失败。",
                        }
                    )

        anchors = _extract_anchor_phrases(plan_text)
        if anchors:
            missing_anchors = [a for a in anchors if a not in chapter_text]
            if missing_anchors:
                failures.append(
                    {
                        "rule": "anchor_phrase_missing_in_manuscript",
                        "evidence": f"以下 anchor_phrase 未在正文中原样出现：{', '.join(missing_anchors[:10])}",
                        "fix": "修订正文将每条 anchor_phrase 自然融入；或重跑 Step2 并在 Drafter 中强制 anchor_phrase 原样出现。",
                    }
                )
        else:
            warnings.append(
                {
                    "topic": "anchor_phrase_missing_in_plan",
                    "evidence": "章纲未检测到 anchor_phrase 字段。",
                    "suggestion": "建议升级 ChapterPlanner：Scene List 每场景输出 anchor_phrase（8–16 字）以便 QA 可验证。",
                }
            )

        scene_count = _count_scenes(plan_text)
        cost_count = len(re.findall(r"\bcost:\s*\S", plan_text))
        if cost_count > 0 and scene_count > 0 and cost_count < scene_count:
            msg = f"Scene List 场景数≈{scene_count}，但 cost 字段仅 {cost_count} 条。"
            if STRICT_NARRATIVE_QA:
                failures.append(
                    {
                        "rule": "scene_cost_missing",
                        "evidence": msg,
                        "fix": "升级 ChapterPlanner：每个场景补齐 cost（具体代价）。",
                    }
                )
            else:
                warnings.append(
                    {
                        "topic": "scene_cost_missing",
                        "evidence": msg,
                        "suggestion": "建议升级 ChapterPlanner：每场景写 cost（代价），后续可设为硬失败。",
                    }
                )

        # --- 原有 open_loops / 占位符检查 ---
        if len(planned) < 2:
            failures.append(
                {
                    "rule": "open_loops_advance_min_2",
                    "evidence": f"章纲中标注推进的 open_loops 数量不足：{planned}",
                    "fix": "更新章纲，至少推进 2 条 open_loops（明确写出 id）。",
                }
            )
        plan_placeholder_issues = _scan_placeholders(plan_text) + _scan_empty_plan_fields(plan_text)
        if plan_placeholder_issues:
            failures.append(
                {
                    "rule": "chapter_plan_placeholders",
                    "evidence": f"chapter_plan 出现占位符或空字段：{', '.join(plan_placeholder_issues)}",
                    "fix": "回到 Step1 重新生成 chapter_plan，禁止 ???/TBD/待补全/TODO，并补齐关键字段。",
                }
            )
            placeholder_report_lines.append(f"- chapter_plan: FAIL（{', '.join(plan_placeholder_issues)}）")
        else:
            placeholder_report_lines.append("- chapter_plan: PASS")

    summary_text = read_text(summary_path) if summary_path.exists() else ""
    rolling_path = root / "recap" / "rolling_recap.md"
    rolling_text = read_text(rolling_path) if rolling_path.exists() else ""
    patch_text = json.dumps(patch_obj, ensure_ascii=False) if patch_obj is not None else ""

    summary_issues = _scan_placeholders(summary_text) if summary_text else []
    if summary_issues:
        failures.append(
            {
                "rule": "chapter_summary_placeholders",
                "evidence": f"章节摘要出现占位/乱码：{', '.join(summary_issues)}",
                "fix": "重跑 Step3（Archivist），确保摘要为 UTF-8 且无占位符/乱码。",
            }
        )

    rolling_issues = _scan_placeholders(rolling_text) if rolling_text else []
    if rolling_issues:
        failures.append(
            {
                "rule": "rolling_recap_placeholders",
                "evidence": f"滚动摘要出现占位/乱码：{', '.join(rolling_issues)}",
                "fix": "重跑 Step3（Archivist），确保滚动摘要为 UTF-8 且无占位符/乱码。",
            }
        )

    changelog_path = run_dir / "changelog.md"
    if changelog_path.exists():
        changelog_text = read_text(changelog_path)
        changelog_issues = _scan_placeholders(changelog_text)
        if changelog_issues:
            failures.append(
                {
                    "rule": "changelog_placeholders",
                    "evidence": f"changelog 出现占位符：{', '.join(changelog_issues)}",
                    "fix": "回到 Step3 重新生成 changelog，禁止 ???/TBD/待补全/TODO。",
                }
            )
            placeholder_report_lines.append(f"- changelog: FAIL（{', '.join(changelog_issues)}）")
        else:
            placeholder_report_lines.append("- changelog: PASS")
    else:
        placeholder_report_lines.append("- changelog: 缺失")

    if planned:
        if not summary_path.exists():
            failures.append(
                {
                    "rule": "artifact_missing: chapter_summary",
                    "evidence": f"缺失文件：{summary_path.as_posix()}",
                    "fix": "先写 `recap/chapter_summaries/chNNN.md`。",
                }
            )
        if patch_obj is None:
            failures.append(
                {
                    "rule": "artifact_missing_or_invalid: state_patch",
                    "evidence": f"缺失或无效：{patch_path.as_posix()}",
                    "fix": "先生成 `state/state_patch.json`（JSON 对象）。",
                }
            )

        for loop_id in planned:
            if summary_text and (loop_id not in summary_text):
                failures.append(
                    {
                        "rule": "open_loop_evidence_in_summary",
                        "evidence": f"章纲计划推进的 loop '{loop_id}' 未出现在章节摘要中。",
                        "fix": "更新章节摘要，明确该 loop 的推进方式。",
                    }
                )
            if patch_text and (loop_id not in patch_text):
                failures.append(
                    {
                        "rule": "open_loop_evidence_in_state_patch",
                        "evidence": f"章纲计划推进的 loop '{loop_id}' 未出现在 state_patch.json 中。",
                        "fix": "更新 state_patch，写明该 loop 的状态/推进（按 id）。",
                    }
                )

    sync_info = check_artifacts_in_sync(root, run_dir, chap, failures)
    needs_resync = bool(sync_info["missing"] or sync_info["out_of_sync"])
    resync_path = run_dir / "resync_instructions.md"
    if needs_resync:
        resync_text = _build_resync_instructions(args.date, chap, sync_info)
        safe_write_text(resync_path, resync_text + "\n", backup=True)
    elif resync_path.exists():
        existing = read_text(resync_path)
        if "Resync Instructions" in existing or "JSON object" in existing or "delta only" in existing:
            resync_text = _build_resync_instructions(args.date, chap, sync_info)
            safe_write_text(resync_path, resync_text + "\n", backup=True)
    sync_report_lines = sync_info["report_lines"]

    append_like, rewrite_like = _canon_change_watch(root, run_dir)
    if rewrite_like:
        failures.append(
            {
                "rule": "canon_rewrite_detected",
                "evidence": f"检测到 canon 重写：{', '.join(rewrite_like)}",
                "fix": "回滚非预期 canon 改写，或在 QA 报告与 changelog 中明确记录原因与影响。",
            }
        )
    if append_like:
        warnings.append(
            {
                "topic": "canon_changed_append_like",
                "evidence": f"canon 发生追加式变更：{', '.join(append_like)}",
                "suggestion": "在 changelog 记录追加内容，并复核一致性。",
            }
        )

    qa_result = "PASS" if not failures else "FAIL"

    report_lines: list[str] = []
    report_lines.append(f"QA_RESULT: {qa_result}")
    report_lines.append("")
    report_lines.append(f"# QA 报告 — {args.date} — {chap}")
    report_lines.append("")

    report_lines.append("## 硬失败")
    if failures:
        for item in failures:
            report_lines.append(f"- 规则: {item['rule']}")
            report_lines.append(f"  证据: {item['evidence']}")
            report_lines.append(f"  修复: {item['fix']}")
    else:
        report_lines.append("- （无）")

    report_lines.append("")
    report_lines.append("## 警告")
    if warnings:
        for item in warnings:
            report_lines.append(f"- 主题: {item['topic']}")
            report_lines.append(f"  证据: {item['evidence']}")
            report_lines.append(f"  建议: {item['suggestion']}")
    else:
        report_lines.append("- （无）")

    report_lines.append("")
    report_lines.append("## 章节号一致性")
    if infos:
        for info in infos:
            report_lines.append(f"- {info}")
    else:
        report_lines.append("- （无）")

    report_lines.append("")
    report_lines.append("## 正文字数")
    if body_chars is None:
        report_lines.append("- 正文字符数: 无")
    else:
        report_lines.append(f"- 正文字符数: {body_chars}")
        report_lines.append("- 最低要求: 3000")

    report_lines.append("")
    report_lines.append("## open_loops 校验")
    report_lines.append(f"- 章纲标注推进: {planned}")
    report_lines.append(f"- 章纲路径: {plan_path.as_posix()}")
    report_lines.append(f"- 摘要路径: {summary_path.as_posix()}")
    report_lines.append(f"- state_patch 路径: {patch_path.as_posix()}")

    report_lines.append("")
    report_lines.append("## 占位符/问号串检测")
    if placeholder_report_lines:
        report_lines.extend(placeholder_report_lines)
    else:
        report_lines.append("- （无）")

    report_lines.append("")
    report_lines.append("## 归档同步")
    report_lines.extend(sync_report_lines)

    report_lines.append("")
    report_lines.append("## 推荐返工步骤（仅 FAIL 时）")
    report_lines.append("1) 补齐缺失产物（chapter_plan / manuscript / summary / patch）。")
    report_lines.append("2) 确保章纲推进 >=2 条 open_loops，并明确写出 id。")
    report_lines.append("3) 更新 summary + state_patch，让每条计划 loop 的 id 明确出现。")
    report_lines.append(f"4) 重跑：`python tools/continuity_checks.py --date {args.date} --chapter {chap}`")

    safe_write_text(qa_path, "\n".join(report_lines) + "\n", backup=True)
    log.info("wrote qa report: %s (result=%s)", qa_path.as_posix(), qa_result)
    return 0 if qa_result == "PASS" else 1


if __name__ == "__main__":
    raise SystemExit(main())
